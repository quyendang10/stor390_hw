---
title: "HW 2 Student"
author: "Quyen Dang"
date: "10/17/2023"
output: 
  html_document:
    number_sections: true
---

This homework is meant to illustrate the methods of classification algorithms as well as their potential pitfalls.  In class, we demonstrated K-Nearest-Neighbors using the `iris` dataset.  Today I will give you a different subset of this same data, and you will train a KNN classifier.  

```{r}
set.seed(123)
library(class)

df <- data(iris) 

normal <-function(x) {
  (x -min(x))/(max(x)-min(x))   
}

iris_norm <- as.data.frame(lapply(iris[,c(1,2,3,4)], normal))

subset <- c(1:45, 58, 60:70, 82, 94, 110:150)
iris_train <- iris_norm[subset,] 
iris_test <- iris_norm[-subset,] 

iris_target_category <- iris[subset,5]
iris_test_category <- iris[-subset,5]


```

#
Above, I have given you a training-testing partition.  Train the KNN with $K = 5$ on the training data and use this to classify the 50 test observations.  Once you have classified the test observations, create a contingency table -- like we did in class -- to evaluate which observations your algorithm is misclassifying.   

```{r}
set.seed(123)
#STUDENT INPUT
library(class)

# Apply KNN classifier
pr <- knn(iris_train, iris_test, cl = iris_target_category, k = 5)

# Create a contingency table to compare predicted vs actual
tab <- table(pr, iris_test_category)
tab

# Function to calculate accuracy
accuracy <- function(x){
  sum(diag(x)/(sum(rowSums(x)))) * 100
}

# Calculate the accuracy
acc <- accuracy(tab)
acc
```

#

Discuss your results.  If you have done this correctly, you should have a classification error rate that is roughly 20% higher than what we observed in class.  Why is this the case? In particular run a summary of the `iris_test_category` as well as `iris_target_category` and discuss how this plays a role in your answer.  

*STUDENT INPUT* 
The training set has 45 setosa, 14 versicolor, and 41 virginica, while the test set has 5 setosa, 36 versicolor, and 9 virginica. This imbalance, especially the underrepresentation of versicolor in the training set (14) compared to its dominance in the test set (36), leads to higher misclassification of versicolor. The same applies for setosa and virginica, with high representation in the training set and little representation in the test set. As a result, the error rate is 22%, higher than the more balanced dataset we observed in class.

```{r}
# Summarize the iris_test_category and iris_target_category
summary(iris_test_category)
summary(iris_target_category)
```

#

Choice of $K$ can also influence this classifier.  Why would choosing $K = 6$ not be advisable for this data? 

*STUDENT INPUT*
Choosing K = 6 isn't advisable because the KNN classifier works best with odd values of K when working with a three category problem like this one. Using an even K such as 6 increases the chance of ties in the voting process for nearest neighbors, leading to less accurate classifications.

#

Build a github repository to store your homework assignments.  Share the link in this file.  

*STUDENT INPUT*

